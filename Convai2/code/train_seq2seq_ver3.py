import argparse
import torch
from torch.autograd import Variable
import seq2seq_model
import torch.optim as optim
import datetime
import os
import codecs
from tqdm import tqdm
import numpy as np
import pickle
from parse_data_ver4 import *
import generator
import discriminator
import helpers
from seq_gan import *
from tqdm import tqdm
import random

CUDA = torch.cuda.is_available()

VOCAB_SIZE = 5000
MAX_SEQ_LEN = 300
START_LETTER = 0
BATCH_SIZE = 20
MLE_TRAIN_EPOCHS = 1
ADV_TRAIN_EPOCHS = 50
POS_NEG_SAMPLES = 10000

GEN_EMBEDDING_DIM = 32
GEN_HIDDEN_DIM = 10
DIS_EMBEDDING_DIM = 32
DIS_HIDDEN_DIM = 10

def one_epoch(eth, batch_list, model, criterion, optimizer, train=True):
    if train:
        model.train()
    else:
        model.eval()
    total_loss = 0.0
    total_num = 0
    for personas, turn_batch_list in tqdm(batch_list):
        personas = Variable(personas).cuda()
        model.init_persona(personas)

        loss = Variable(torch.zeros(1)).cuda()
        num = Variable(torch.zeros(1)).cuda()
        for batch_xs, batch_ys, n in turn_batch_list:
            batch_xs = Variable(batch_xs).cuda()
            batch_ys = Variable(batch_ys).cuda()
            pred, out = model(batch_xs, batch_ys)
            out = out.view(-1, out.size(2))
            loss = criterion(out, batch_ys.view(-1))
            if train:
                optimizer.zero_grad()
                loss.backward(retain_graph=True)
                torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=0.1)
                optimizer.step()
            #print (out.size(), batch_ys.size())
            #loss += criterion(out, batch_ys.view(-1))
            #num += n
            #del batch_xs, batch_ys, pred, out
            total_loss += loss.data*n
            total_num += n
        #if train:
        #    optimizer.zero_grad()
        #    loss /= num
        #    loss.backward(retain_graph=True)
        #    torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=0.1)
        #    optimizer.step()
        #    #loss(out, xs)
    return total_loss/total_num

def get_batch_list(episodes, step_size=1):
    batch_list = []
    for b_start in tqdm(range(0, len(episodes) - step_size + 1, step_size)):
        personas_your = get_persona_batch(episodes[b_start:b_start+step_size], 1)
        personas_partner = get_persona_batch(episodes[b_start:b_start+step_size], 0)
        turn_batch_list = get_dialog_batches(episodes[b_start:b_start+step])
        batch_list.append((personas_your, personas_partner, turn_batch_list))
    return batch_list

def main(args):
    episodes = split_data(args.data)

    #episodes = episodes[:len(episodes)//30] # for debug
    valid_rate = 0.15
    episodes = np.array(episodes, dtype=object)
    valid_num = int(valid_rate*len(episodes))
    valid_episodes = episodes[:valid_num]
    episodes = episodes[valid_num:]

    vocab2index, index2vocab, embedding_weight, embedding_dim = build_vocab(episodes, args.embedding, 100, train_oov=False)
    episodes_text2index(episodes, vocab2index)
    episodes_text2index(valid_episodes, vocab2index)

    batch_size = args.batch_size
    #batch_list = get_batch_list(episodes, batch_size)
    #valid_batch_list = get_batch_list(valid_episodes, batch_size)

    save_round = 1
    date = datetime.datetime.now().strftime("%d-%H-%M")
    save_path = 'model/model_{}'.format(date)
    print ('save_path = {}'.format(save_path))
    if not os.path.exists(save_path):
        os.makedirs(save_path, exist_ok=True)
    with open(os.path.join(save_path, 'vocab.pickle'), 'wb') as f:
        pickle.dump({'vocab2index':vocab2index, 'index2vocab':index2vocab}, f)
    log_file = codecs.open(os.path.join(save_path, 'log'), 'w')
    embedding_weight = torch.Tensor(embedding_weight)

    #oracle = generator.Generator(GEN_EMBEDDING_DIM, GEN_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA)
    #oracle.load_state_dict(torch.load(oracle_state_dict_path))
    #oracle_samples = torch.load(oracle_samples_path).type(torch.LongTensor)
    # a new oracle can be generated by passing oracle_init=True in the generator constructor
    # samples for the new oracle can be generated using helpers.batchwise_sample()

    gen = generator.Generator(embedding_dim, GEN_HIDDEN_DIM, len(vocab2index), MAX_SEQ_LEN, embedding_weight, gpu=CUDA)
    dis = discriminator.Discriminator(embedding_dim, DIS_HIDDEN_DIM, len(vocab2index), MAX_SEQ_LEN, embedding_weight, gpu=CUDA)

    if CUDA:
        #oracle = oracle.cuda()
        gen = gen.cuda()
        dis = dis.cuda()
        #oracle_samples = oracle_samples.cuda()
    #for parameters in gen.parameters():
    #    print(parameters)

    # GENERATOR MLE TRAINING
    print('Starting Generator MLE Training...')
    gen_optimizer = optim.Adam(filter(lambda p: p.requires_grad, gen.parameters()), lr=1e-2)
    train_generator_MLE(gen, gen_optimizer, episodes, valid_episodes, batch_size, MLE_TRAIN_EPOCHS)

    # torch.save(gen.state_dict(), pretrained_gen_path)
    # gen.load_state_dict(torch.load(pretrained_gen_path))

    # PRETRAIN DISCRIMINATOR
    print('\nStarting Discriminator Training...')
    dis_optimizer = optim.Adagrad(filter(lambda p: p.requires_grad, dis.parameters()))
    train_discriminator(dis, dis_optimizer, episodes, valid_episodes, gen, batch_size, 1, 1)

    # torch.save(dis.state_dict(), pretrained_dis_path)
    # dis.load_state_dict(torch.load(pretrained_dis_path))

    # ADVERSARIAL TRAINING
    #print('\nStarting Adversarial Training...')
    #oracle_loss = helpers.batchwise_oracle_nll(gen, oracle, POS_NEG_SAMPLES, BATCH_SIZE, MAX_SEQ_LEN,
                                               #start_letter=START_LETTER, gpu=CUDA)
    #print('\nInitial Oracle Sample Loss : %.4f' % oracle_loss)

    for epoch in range(ADV_TRAIN_EPOCHS):
        print('\n--------\nEPOCH %d\n--------' % (epoch+1))
        # TRAIN GENERATOR
        print('\nAdversarial Training Generator : ', end='')
        sys.stdout.flush()
        train_generator_PG(gen, gen_optimizer, dis, batch_size, episodes, 1, 20)

        # TRAIN DISCRIMINATOR
        print('\nAdversarial Training Discriminator : ')
        train_discriminator(dis, dis_optimizer, episodes, valid_episodes, gen, batch_size, 1, 1)
    #model = seq2seq_model.Seq2Seq({'num_embeddings':len(index2vocab), 'embedding_dim':embedding_dim, 'embdding_weight':embedding_weight,
                                #   "rnn_class":torch.nn.GRU, 'hidden_size':128, 'num_layers':2, 'dropout':0.5, 'bidirectional':True,
                                #   "history_size": 256*2, 'persona_size': embedding_dim}).cuda()
    #criterion = torch.nn.CrossEntropyLoss(ignore_index=idx_PAD, size_average=True).cuda()
    #optimizer = optim.Adam(model.parameters())
    #part_num = 2
    #part_size = len(batch_list)//part_num + 1
    #for e in range(100):
    #    for p in range(part_num):
    #        loss = one_epoch(e, batch_list[p*part_size:(p+1)*part_size], model, criterion, optimizer, train=True)
    #        print ('episodes = {}, training_loss = {}'.format(e, loss))
    #        print ('episodes = {}, training_loss = {}'.format(e, loss), file=log_file)

    #        loss = one_epoch(e, valid_batch_list, model, criterion, optimizer, train=False)
    #        print ('episodes = {}, valid_loss = {}'.format(e, loss))
    #        print ('episodes = {}, valid_loss = {}'.format(e, loss), file=log_file)
    #    if e % save_round == save_round - 1:
    #        with open(os.path.join(save_path, 'model_{}'.format(e)), 'wb') as f:
    #                torch.save(model.state_dict(), f)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--batch-size', type=int, default='20')
    parser.add_argument('--data', type=str, default="train_both_original_no_cands.txt")
    parser.add_argument('--embedding', type=str, default="glove.6B.100d.txt")

    main( parser.parse_args() )

